# Step-1 : importing the libraries

import pandas as pd
import numpy as np
from pandas_datareader import data
import nsepy as ns
import nsepy.derivatives as nsd
from nsepy import get_history
from datetime import date
from nsepy.derivatives import get_expiry_date
from datetime import date , datetime
from numpy import log, polyfit, sqrt , std, subtract
import statsmodels.tsa.stattools as ts
import statsmodels.api as sm
import matplotlib.pyplot as plt
import pprint
import iteration_utilities
from iteration_utilities import duplicates
from iteration_utilities import unique_everseen

# Step -2 : Reading the text file of all the symbols- 'FnO_30.txt'

stocks= pd.read_csv('FnO_30.txt', delimiter= "\n")

#setting up empty list to hold the stock tickers
stocks_list= []

#iterate through the pandas dataframe of tickers and appned them in empty list
for symbol in stocks['Symbol']:
    stocks_list.append(symbol)

len(stocks_list) # the length of the list=> there are 30 symbols given

stocks_list[:5] # first five symbols of the list

# Step-3: Creating pairs :
## Using the permutations feature of python's itertools library: (in order to avoid the repeatition of any pair)

# importing the required library
import itertools
from itertools import combinations

# Generating the unique pairs & storing in the varible 'symbol_pairs'
symbol_pairs= list(combinations(stocks['Symbol'],2))
print(symbol_pairs)

list(unique_everseen(duplicates(symbol_pairs))) # to check if there are any duplicates present

type(symbol_pairs)  # type of the list generated by above code

len(symbol_pairs) ## the number of unique pairs => 435

# Step-4: Collecting the data for each futures stock "Symbol Pair" element from 'NSE' & storing as dataframe

### columns for the data frame which will store the below ADF statitics for all co-integrated pairs
cols = ['Pair(y)','Pair(x)','ADF Test Statistics','ADF p-value',
               'ADF at 1%','ADF at 5%','ADF at 10%', 'Status']
lst =[]

### loop to download the data and then check for co-integration for each symbol-pair
for i in symbol_pairs:
    try:
        
        symb = i
        print("FOR THE PAIR => ",symb)
        # downloading the historical data from NSE:
        y= ns.get_history(symbol= symb[0], start= date(2016,10,10),
                                     end= date(2018,10,10), futures= True,
                                     expiry_date= nsd.get_expiry_date(2018,10))
        x= ns.get_history(symbol= symb[1], start= date(2016,10,10),
                                     end= date(2018,10,10), futures= True,
                                     expiry_date= nsd.get_expiry_date(2018,10))

        #to make sure Dataframes are the same length
        min_date= max(df.dropna().index[0] for df in [y,x])
        max_date= min(df.dropna().index[-1] for df in [y,x])

        y= y[(y.index>= min_date) & (y.index <= max_date)]
        x= x[(x.index>= min_date) & (x.index <= max_date)]

        # to run ordinary Least Squares regression to  fing hedge ratio and then to create spread series
        df1= pd.DataFrame({'y':y['Close'],'x':x['Close']})
        est= sm.OLS(df1.y,df1.x)
        est= est.fit()
        df1['hr']= -est.params[0]
        df1['spread']= df1.y + (df1.x * df1.hr)

        #plt.plot(df1.spread)
        #plt.show()

        ## ADF test on the pair
        cadf= ts.adfuller(df1.spread)
        #print('ADF test Statistic = ', cadf[0])
        #print('ADF p-value = ', cadf[1])
        #print('ADF 1%, 5% and 10% test statistics = ', cadf[4])

        ## checking whether the pair is co-integrated and printing the result:
        if cadf[0]<=cadf[4]['10%'] and cadf[1]<= 0.05:
            status = True
            lst.append([symb[0],symb[1],cadf[0],cadf[1],cadf[4]['1%'],
                       cadf[4]['5%'],cadf[4]['10%'], "Clear at 95% CL"])
        else:
            status = False
            print("CONCLUSION:NOT co-integrated")
            
    except:
        continue
    

### DATA FRAME OF ALL ADF STATISTICS 
ADF_data = pd.DataFrame(lst, columns = cols)
ADF_data

len(ADF_data)  ## length of the ADF dataframe

# Exporting dataframes to csv file for reports

#ADF_data.to_csv('ADF_file.csv', header = True)
#sbm_list= pd.DataFrame(symbol_pairs)
#sbm_list.to_csv('Symbol_pairs_list.csv')

#Exported already

# Hence 29 out of 435 pairs cleared the Augmented Dickey Fuller test 

final_pairs= [ADF_data['Pair(y)'],ADF_data['Pair(x)']]
type(final_pairs) 
print(final_pairs[0][1],final_pairs[1][0])

type(final_pairs)
len(final_pairs)##  Length of List of final pairs

# Setting path for pdf file to store plots for final Pairs

image_pdf = '/home/paras/Downloads/plots.pdf'
import matplotlib.backends.backend_pdf
import scipy 
from scipy import stats
import seaborn as sns

# Step - 5: Applying Linear regression on the Final Pairs and BACKTESTING 


### columns for the data frame which will store the below statitical measures:

cols_new = ['Pair(y)','Pair(x)','Mean','Standard Deviation',
           'Beta Coefficient', 'Intercept','P-value','P-value Interpretation',
            'STD_Residuals','Halflife','CAGR % at (2,-2) Zscore',
            'Sharpe Ratio at (2,-2) Zscore','CAGR % at (2.5,-2.5) Zscore',
           'Sharpe Ratio at (2.5,-2.5) Zscore',]
lst_new =[]


### loop to download the data and then Backtesting on each final-pair
for t in range(0,30):
    try:
        #pdf = matplotlib.backends.backend_pdf.PdfPages(plots.pdf)
        #fig = plt.figure(figSize=[15,15])  #
        pair = [final_pairs[0][t],final_pairs[1][t]]
        print("FOR THE PAIR => ",pair)
        plt.title('pair')
        
        # downloading the historical data from NSE:
        y= ns.get_history(symbol= pair[0], start= date(2016,10,10),
                                     end= date(2018,10,10), futures= True,
                                     expiry_date= nsd.get_expiry_date(2018,10))
        x= ns.get_history(symbol= pair[1], start= date(2016,10,10),
                                     end= date(2018,10,10), futures= True,
                                     expiry_date= nsd.get_expiry_date(2018,10))
        
        #to make sure Dataframes are the same length
        min_date= max(df.dropna().index[0] for df in [y,x])
        max_date= min(df.dropna().index[-1] for df in [y,x])

        y= y[(y.index>= min_date) & (y.index <= max_date)]
        x= x[(x.index>= min_date) & (x.index <= max_date)]
        #print("index is correct")
        
        #to run ordinary Least Squares regression to  find hedge ratio and then to create spread series
        
        import statsmodels.tsa.stattools as ts
        import statsmodels.api as sm
        
        df1= pd.DataFrame({'y':y['Close'],'x':x['Close']})
        est= sm.OLS(df1.y,df1.x)
        est = est.fit()
        df1['hr']= -est.params[0]
        df1['spread']= df1.y + (df1.x * df1.hr)
        
        #print("spread is created")
        
        #plt.plot(y.price,label=symbList[0])
        #plt.plot(x.price,label=symbList[1])
        #plt.ylabel('Price')
        #plt.xlabel('Time')
        #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
    
    
        #############For joint plot ##########################################
        
        #sns.jointplot(y.price, x.price ,color='b')
        #plt.title('Joint Plot')
        
        
        ###--- for 'Spread' plot-----
        
        #plt.plot(df1.spread)
        #plt.xlabel('pair(x)')
        #plt.ylabel('pair(y')
        #plt.title('Spread plot')
        
        #Run OLS regression on spread series and lagged version of itself
        
        spread_lag = df1.spread.shift(1)
        spread_lag.iloc[0] = spread_lag.iloc[1]
        spread_ret = df1.spread - spread_lag
        spread_ret.iloc[0] = spread_ret.iloc[1]
        spread_lag2 = sm.add_constant(spread_lag)

        model = sm.OLS(spread_ret,spread_lag2)
        res = model.fit()
        
        ## Calculating Halflife  
        halflife = round((-np.log(2) / res.params[1]),0)
        
        ####---- to find mean of the spread series:
        mean = df1.spread.mean()
        stand= df1.spread.std()
        resi = res.resid.std()
        p_value = res.pvalues[1]
       
        ##p-value interpretation check:
        if p_value <= 0.05:
            p_status = "Model is statistically significant at 95% CL"
        else:
            p_status = "Not significant at 95% CL"
        
        
        ##### -- Z-score calculations:
        #to calculate the Z-Score by using a rolling window for the mean and standard deviation, set as the half-life 
        #to save either committing a look-forward bias by using the mean across the whole period, or from choosing an arbitrary look-back window 
        #that would need to be optimised and could lead to data-mining bias.
        
        meanSpread = df1.spread.rolling(window=int(halflife)).mean()
        stdSpread = df1.spread.rolling(window=int(halflife)).std()
        
        df1['zScore']= (df1.spread-meanSpread)/ stdSpread
        
        
        #lt.plot(df1['zScore'])
        #lt.title('zScore Plot')
        
        ### Backtesting done on both z-score (above and below (+2 & -2) &(2.5 &-2.5)
        level = [[2,-2],[2.5,-2.5]]
        ratio= []
        for i in range(0,2):
            entryZscore = level[i][0]
            exitZscore = level[i][1]
            print(entryZscore, exitZscore)
            

            #set up num units long             
            df1['long entry'] = ((df1.zScore < - entryZscore) & ( df1.zScore.shift(1) > - entryZscore))
            df1['long exit'] = ((df1.zScore > - exitZscore) & (df1.zScore.shift(1) < - exitZscore)) 
            df1['num units long'] = np.nan 
            df1.loc[df1['long entry'],'num units long'] = 1 
            df1.loc[df1['long exit'],'num units long'] = 0 
            df1['num units long'][0] = 0 
            df1['num units long'] = df1['num units long'].fillna(method='pad') 

            #set up num units short 
            df1['short entry'] = ((df1.zScore >  entryZscore) & ( df1.zScore.shift(1) < entryZscore))
            df1['short exit'] = ((df1.zScore < exitZscore) & (df1.zScore.shift(1) > exitZscore))
            df1.loc[df1['short entry'],'num units short'] = -1
            df1.loc[df1['short exit'],'num units short'] = 0
            df1['num units short'][0] = 0
            df1['num units short'] = df1['num units short'].fillna(method='pad')
        
            #The daily portfolio returns are then cumulatively 
            #added to generate an equity curve, held in “cum rets”.
        
            df1['numUnits'] = df1['num units long'] + df1['num units short']
            df1['spread pct ch'] = (df1['spread'] - df1['spread'].shift(1)) / ((df1['x'] * abs(df1['hr'])) + df1['y'])
            df1['port rets'] = df1['spread pct ch'] * df1['numUnits'].shift(1)
        
            df1['cum rets'] = df1['port rets'].cumsum()
            df1['cum rets'] = df1['cum rets'] + 1
            
                
            #to plot the portfolio equity curve as follows:
            #plt.plot(df1['cum rets'])
            #plt.xlabel(i[1])
            #plt.ylabel(i[0])
            #plt.title('Portfolio Equity curve')
            #plt.show()
            
            #calculate the Sharpe Ratio and the Compound Annual Growth Rate (CAGR):
            try:
                sharpe = ((df1['port rets'].mean()/df1['port rets'].std()) * sqrt(252))  
            except ZeroDivisionError:
                sharpe = 0.0
            
            start_val = 1
            end_val = df1['cum rets'].iat[-1]
    
            start_date = df1.iloc[0].name
            end_date = df1.iloc[-1].name
            days = (end_date - start_date).days
    
            CAGR = round(((float(end_val) / float(start_val)) ** (252.0/days)) - 1,4)

            print(CAGR)
            Cagr_per= "The ZScore at"+ str(entryZscore) +"&"+str(exitZscore) +"the CAGR % is " + str(CAGR*100)
            Sharpe_per="The ZScore at"+str(entryZscore)+"&"+ str(exitZscore) + "the Sharpe ratio is "+ str(round(sharpe,2))
            print(Cagr_per)
            lt = [Cagr_per,Sharpe_per]
            ratio.append(lt)
        
        stat= []
        stat= [pair[0],pair[1],mean,stand,
                                    res.params[1],
                                    res.params[0],res.pvalues[1],
                                    p_status,resi,halflife,ratio[0][0],
                                    ratio[0][1],ratio[1][0],ratio[1][1]]
        #print("list is updated:",stat)
        lst_new.append(stat)
        #print(lst_new)
    except:
        continue

### DATA FRAME OF ALL MODEL STATISTICS 
Model_data = pd.DataFrame(lst_new,columns = cols_new)
Model_data 

# Step- 6: Exporting all statistical measures to a csv file

## Exporting the final Statistics:

#Model_data.to_csv("Final_statistic.csv")
#exported already
